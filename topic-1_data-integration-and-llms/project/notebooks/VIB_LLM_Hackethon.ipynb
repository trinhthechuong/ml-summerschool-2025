{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37495fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import sys\n",
    "sys.path.append(\"/Users/SJp/Documents/project_local/VIB-LLM-SS/ml-summerschool-2025/topic-1_data-integration-and-llms/project\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "import src.VIB_project_functions as FF\n",
    "importlib.reload(FF)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from typing import Optional, List, Union\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from typing import List, Optional\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "from json import JSONEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31a4fa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_path  = \"/Users/SJp/Documents/project_local/VIB-LLM-SS/ml-summerschool-2025/topic-1_data-integration-and-llms/project\"\n",
    "# Load data\n",
    "# cytokines = pd.read_csv(f\"{global_path}/data/Cytokines.csv\", header=0, index_col=0).transpose()\n",
    "# clinical = pd.read_csv(f\"{global_path}/data/Clinical.csv\", header=0, index_col=0).transpose()\n",
    "# proteomics = pd.read_csv(f\"{global_path}/data/MyelomaCells_proteomics.csv\", header=0, index_col=0).transpose()\n",
    "# drugresponse = pd.read_csv(f\"{global_path}/data/DrugResponse.csv\", header=0, index_col=0).transpose().dropna()\n",
    " \n",
    "# # make sure all the above dataframes have the same patients in the same order\n",
    "# common_patients = set(clinical.index) & set(cytokines.index) & set(proteomics.index) & set(drugresponse.index)\n",
    "# common_patients = sorted(list(common_patients))\n",
    "# clinical = clinical.loc[common_patients].sort_index()\n",
    "# cytokines = cytokines.loc[common_patients].sort_index()\n",
    "# proteomics = proteomics.loc[common_patients].sort_index()\n",
    "# drugresponse = drugresponse.loc[common_patients].sort_index()\n",
    "# clinical_numeric = pd.get_dummies(clinical)\n",
    "\n",
    "# drug_classes = pd.read_csv(f\"{global_path}/data/drugresponse_classes.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f335c1",
   "metadata": {},
   "source": [
    "## ML model to predict the drug response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00b9165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results, patient_explanations=FF.run_drug_classification(\n",
    "#     proteomics, cytokines, clinical_numeric, drug_classes, \n",
    "#     output_dir=f\"{global_path}/results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "451016b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FF.save_patient_graphs_json(patient_explanations, results, clinical, outdir=f\"{global_path}/results/patient_graphs_json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd7e75a",
   "metadata": {},
   "source": [
    "## LLM empowered results integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d590b5df",
   "metadata": {},
   "source": [
    "### Generate the summary report of drug response prediction from ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aa9f05",
   "metadata": {},
   "source": [
    "* Reading patient JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4869b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from typing import Optional, List, Union\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from typing import List, Optional\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "from json import JSONEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cb12eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick via provider:model string â†’ works across providers\n",
    "llm = init_chat_model(model=\"gemini-2.5-flash\",\n",
    "                      model_provider=\"google_genai\",\n",
    "                      temperature=0.2)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a biomedical-AI assistant that interprets predictions from a AI-powered predictive model for clinicians.\n",
    "You are given a JSON with some information regarding the patient and 2 drugs. In the JSON, you are given:\n",
    "- Patient ID\n",
    "- Disease type\n",
    "- For each drug, you will be given:\n",
    "    - The drug name\n",
    "    - The predicted class. This class can be one of three options: \n",
    "        - No effect if the drug is predicted as having no effect on treating the patient disease\n",
    "        - Positive response if the drug is predicted as having a positive effect on treating the patient disease\n",
    "        - Adverse effects if the drug is predicted as having a negative effect on the patient disease\n",
    "    - Each predicted class has an associated probability\n",
    "    - Each predicted class has associated features, that are responsible for the prediction. To reflect the importance of these features on the prediction we have the SHAP values. We have the top positive SHAP values and the top negative SHAP values.\n",
    "\n",
    "Taking into account this JSON and the information explained above, I want you as a smart biomedical-AI assistant to pick the best of the two drugs. I don't want you to talk about the accuracy of the predicition for more than one sentence.\n",
    "Once you have  picked the best drug for my patient, I want you to write a small report on the chosen drug, please include both positive an negative points about the drug, make it as straigthforward as possible (a maximum of 10 bullet points in total), and targeted towards clinicians.\n",
    "After that, please write a short paragraph about the features involved in the decision making process, and look in the litterature for information about the relationship between these features and the disease the patient has.\"\"\"),\n",
    "    (\"human\", \"{JSON_input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "339a1ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to your JSON file (e.g., patient PKG or classification output)\n",
    "json_path = f\"{global_path}/results/MM082.json\"\n",
    "\n",
    "with open(json_path, \"r\") as f:\n",
    "    patient_json = json.load(f)\n",
    "\n",
    "patient_id = \"MM082\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45dbdaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ChatGoogleGenerativeAIError",
     "evalue": "Invalid argument provided to Gemini: 400 * GenerateContentRequest.contents: contents is not specified\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgument\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project_local/VIB-LLM-SS/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:216\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.FailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project_local/VIB-LLM-SS/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:868\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    867\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project_local/VIB-LLM-SS/.venv/lib/python3.13/site-packages/google/api_core/gapic_v1/method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project_local/VIB-LLM-SS/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project_local/VIB-LLM-SS/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project_local/VIB-LLM-SS/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project_local/VIB-LLM-SS/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project_local/VIB-LLM-SS/.venv/lib/python3.13/site-packages/google/api_core/timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project_local/VIB-LLM-SS/.venv/lib/python3.13/site-packages/google/api_core/grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mInvalidArgument\u001b[39m: 400 * GenerateContentRequest.contents: contents is not specified\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m parsing_llm = prompt | llm | parser\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# if `drug_text` is an AIMessage, use .content; otherwise pass the raw string\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m result1 = \u001b[43mparsing_llm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(result1)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# class MyEncoder(JSONEncoder):\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m#     def default(self, o):\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m#         return o.__dict__\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# with open(\".json\", \"w\") as f:\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m#     json.dump(MyEncoder().encode(result), f, indent=2)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project_local/VIB-LLM-SS/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:3049\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3047\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3048\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3049\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3050\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3051\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project_local/VIB-LLM-SS/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1488\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.invoke\u001b[39m\u001b[34m(self, input, config, code_execution, stop, **kwargs)\u001b[39m\n\u001b[32m   1483\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1484\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1485\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTools are already defined.code_execution tool can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be defined\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1486\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1488\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project_local/VIB-LLM-SS/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:383\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    371\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    373\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    378\u001b[39m     **kwargs: Any,\n\u001b[32m    379\u001b[39m ) -> BaseMessage:\n\u001b[32m    380\u001b[39m     config = ensure_config(config)\n\u001b[32m    381\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    382\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    393\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project_local/VIB-LLM-SS/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1006\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    997\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    998\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    999\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1003\u001b[39m     **kwargs: Any,\n\u001b[32m   1004\u001b[39m ) -> LLMResult:\n\u001b[32m   1005\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1006\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project_local/VIB-LLM-SS/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:825\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    823\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    824\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    831\u001b[39m         )\n\u001b[32m    832\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    833\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project_local/VIB-LLM-SS/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1072\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1070\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1076\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project_local/VIB-LLM-SS/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1595\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   1568\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m   1569\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1570\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1581\u001b[39m     **kwargs: Any,\n\u001b[32m   1582\u001b[39m ) -> ChatResult:\n\u001b[32m   1583\u001b[39m     request = \u001b[38;5;28mself\u001b[39m._prepare_request(\n\u001b[32m   1584\u001b[39m         messages,\n\u001b[32m   1585\u001b[39m         stop=stop,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1593\u001b[39m         **kwargs,\n\u001b[32m   1594\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1595\u001b[39m     response: GenerateContentResponse = \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1596\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1597\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1598\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1599\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1600\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1601\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project_local/VIB-LLM-SS/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:247\u001b[39m, in \u001b[36m_chat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    240\u001b[39m params = (\n\u001b[32m    241\u001b[39m     {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _allowed_params_prediction_service}\n\u001b[32m    242\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (request := kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   (...)\u001b[39m\u001b[32m    245\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m kwargs\n\u001b[32m    246\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project_local/VIB-LLM-SS/.venv/lib/python3.13/site-packages/tenacity/__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project_local/VIB-LLM-SS/.venv/lib/python3.13/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project_local/VIB-LLM-SS/.venv/lib/python3.13/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project_local/VIB-LLM-SS/.venv/lib/python3.13/site-packages/tenacity/__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project_local/VIB-LLM-SS/.venv/lib/python3.13/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/project_local/VIB-LLM-SS/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:227\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    224\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.InvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[32m    228\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    229\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.ResourceExhausted \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    231\u001b[39m     \u001b[38;5;66;03m# Handle quota-exceeded error with recommended retry delay\u001b[39;00m\n\u001b[32m    232\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mretry_after\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m e.retry_after < kwargs.get(\n\u001b[32m    233\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwait_exponential_max\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m60.0\u001b[39m\n\u001b[32m    234\u001b[39m     ):\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m: Invalid argument provided to Gemini: 400 * GenerateContentRequest.contents: contents is not specified\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"JSON_input\": patient_json})\n",
    "print(response)\n",
    "\n",
    "class model_prediction_report(BaseModel):\n",
    "    patient_ID: Optional[Union[str, int]]  # \"555-1234\", 5551234, or None\n",
    "    disease_type: str = Field(description=\"e.g., Melanoma\")\n",
    "    decision_making_process: str \n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=model_prediction_report)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Extract per schema:\\n{format_instructions}\"),\n",
    "    (\"human\", \"{text}\"),\n",
    "]).partial(format_instructions=format_instructions)\n",
    "\n",
    "parsing_llm = prompt | llm | parser\n",
    "\n",
    "# if `drug_text` is an AIMessage, use .content; otherwise pass the raw string\n",
    "result1 = parsing_llm.invoke({\"text\": response})\n",
    "print(result1)\n",
    "\n",
    "# class MyEncoder(JSONEncoder):\n",
    "#     def default(self, o):\n",
    "#         return o.__dict__\n",
    "# MyEncoder().encode(result)\n",
    "\n",
    "# with open(\".json\", \"w\") as f:\n",
    "#     json.dump(MyEncoder().encode(result), f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df69d11b",
   "metadata": {},
   "source": [
    "### Functional Analysis of SHAP Features by Predicted Drug Response Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab127f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SJp/Documents/project_local/VIB-LLM-SS/ml-summerschool-2025/topic-1_data-integration-and-llms/project/src/GSEA_bioassistant.py:90: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  enrichr_query({\"gene_list\": list(set(feats[\"positive\"]))}) if feats[\"positive\"] else None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Drugs Predicted as no_effect\n",
      "The positive SHAP features, which support a \"no_effect\" prediction, are primarily associated with general cellular regulatory processes and T-cell biology. These include the regulation of cell-cell adhesion (integrin-mediated), posttranscriptional regulation of gene expression, and protein-containing complex assembly. Additionally, several terms point to the regulation of immune responses, specifically the positive regulation of T cell proliferation and activation. There's also a mention of substantia nigra development and cellular response to indole-3-methanol.\n",
      "\n",
      "Conversely, the negative SHAP features, which argue against a \"no_effect\" prediction, strongly indicate active and specific immune responses. These features highlight neutrophil-mediated immunity, including neutrophil degranulation and activation. They also emphasize the positive regulation of T cell mediated cytotoxicity and immunity, as well as the interferon-gamma-mediated signaling pathway. Other terms include the regulation of actin filament-based processes and the regulation of neuron death.\n",
      "\n",
      "In summary, the \"no_effect\" prediction is supported by pathways involved in general cellular maintenance and controlled T-cell activity, while it is opposed by pathways indicative of robust innate and adaptive immune responses, particularly those involving neutrophils and cytotoxic T cells, along with interferon signaling.\n",
      "\n",
      "### Drugs Predicted as positive_effect\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# notebook.ipynb\n",
    "import json\n",
    "from src.GSEA_bioassistant import get_llm_with_tools, get_prompt_chain, analyze_patient, save_patient_summary_html\n",
    "# --- Setup LLM + chain ---\n",
    "llm_with_tools = get_llm_with_tools()\n",
    "chain = get_prompt_chain(llm_with_tools)\n",
    "\n",
    "# --- Analyze patient ---\n",
    "results, summaries = analyze_patient(patient_json, patient_id, chain)\n",
    "\n",
    "# --- Show results ---\n",
    "for cls, summary in summaries.items():\n",
    "    print(f\"\\n### Drugs Predicted as {cls}\")\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b80288c",
   "metadata": {},
   "source": [
    "### **Open FDA Drug Information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b86cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ibuprofen\n",
      "\n",
      "=== purpose ===\n",
      "Purpose Pain reliever/fever reducer\n",
      "\n",
      "=== indications_and_usage ===\n",
      "Uses temporarily relieves minor aches and pains due to: headache toothache backache menstrual cramps the common cold muscular aches minor pain of arthritis temporarily reduces fever\n",
      "\n",
      "=== warnings ===\n",
      "Warnings Allergy alert: Ibuprofen may cause a severe allergic reaction, especially in people allergic to aspirin. Symptoms may include: rash facial swelling asthma (wheezing) hives skin reddening shock blisters If an allergic reaction occurs, stop use and seek medical help right away. Stomach bleeding warning: This product contains an NSAID, which may cause severe stomach bleeding. The chance is higher if you: take more or for a longer time than directed take a blood thinning (anticoagulant) or steroid drug have had stomach ulcers or bleeding problems have 3 or more alcoholic drinks every day while using this product are age 60 or older take other drugs containing prescription or nonprescription NSAIDs [aspirin, ibuprofen, naproxen, or others] Heart attack and stroke warning: NSAIDs, excep\n",
      "\n",
      "=== dosage_and_administration ===\n",
      "Directions do not take more than directed the smallest effective dose should be used adults and children 12 years and over: take 1 tablet every 4 to 6 hours while symptoms persist if pain or fever does not respond to 1 tablet, 2 tablets may be used do not exceed 6 tablets in 24 hours, unless directed by a doctor children under 12 years: ask a doctor\n"
     ]
    }
   ],
   "source": [
    "import requests, urllib.parse\n",
    "\n",
    "def get_openfda_label(ingredient):\n",
    "    # Standardize ingredient name\n",
    "    ingredient = urllib.parse.quote(ingredient)\n",
    "    print(ingredient)\n",
    "    base = \"https://api.fda.gov/drug/label.json\"\n",
    "    q = f'openfda.substance_name:\"{ingredient}\"'\n",
    "    r = requests.get(base, params={\"search\": q, \"limit\": 1})\n",
    "    r.raise_for_status()\n",
    "    res = r.json().get(\"results\", [])\n",
    "    if not res:\n",
    "        q2 = f'openfda.brand_name:\"{ingredient}\"'\n",
    "        r = requests.get(base, params={\"search\": q2, \"limit\": 1})\n",
    "        r.raise_for_status()\n",
    "        res = r.json().get(\"results\", [])\n",
    "        if not res:\n",
    "            return None\n",
    "    return res[0]\n",
    "\n",
    "item = get_openfda_label(\"ibuprofen\")\n",
    "\n",
    "if item:\n",
    "    for key in (\"purpose\",\"indications_and_usage\", \"adverse_reactions\", \"warnings\", \"dosage_and_administration\"):\n",
    "        if key in item:\n",
    "            print(f\"\\n=== {key} ===\\n{item[key][0][:800]}\")\n",
    "else:\n",
    "    print(\"Cannot find record on openFDA for that name.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977032dd",
   "metadata": {},
   "source": [
    "### Combine results in one JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1730142b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote combined JSON â†’ /Users/SJp/Documents/project_local/VIB-LLM-SS/ml-summerschool-2025/topic-1_data-integration-and-llms/project/combined/MM082_combined.json\n"
     ]
    }
   ],
   "source": [
    "# --- Combine everything into one JSON and save ---\n",
    "from datetime import datetime\n",
    "\n",
    "def _as_dict(x):\n",
    "    \"\"\"Make objects JSON-serializable.\"\"\"\n",
    "    if x is None:\n",
    "        return None\n",
    "    if hasattr(x, \"model_dump\"):\n",
    "        return x.model_dump()\n",
    "    if hasattr(x, \"dict\"):\n",
    "        return x.dict()\n",
    "    if hasattr(x, \"to_dict\"):  # pandas DataFrame, Series, etc.\n",
    "        return x.to_dict()\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return [_as_dict(i) for i in x]\n",
    "    if isinstance(x, dict):\n",
    "        return {k: _as_dict(v) for k, v in x.items()}\n",
    "    return x  # assume it's a primitive type\n",
    "\n",
    "\n",
    "combined = {\n",
    "    \"patient_id\": patient_id,\n",
    "    \"disease_type\": (\n",
    "        getattr(result1, \"disease_type\", None)\n",
    "        or patient_json.get(\"disease_type\")\n",
    "    ),\n",
    "    \"models\": {\n",
    "        \"chat_model\": \"gemini-2.5-flash\",\n",
    "        \"temperature\": 0.2,\n",
    "    },\n",
    "    \"assistant_free_text_report\": (\n",
    "        response.content if hasattr(response, \"content\") else str(response)\n",
    "    ),\n",
    "    \"assistant_structured_report\": _as_dict(result1),\n",
    "    \"prediction_pipeline\": {   # handles DataFrames\n",
    "        \"summaries\": _as_dict(summaries),  # handles dict of strings / dfs\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# Save next to your other outputs\n",
    "os.makedirs(os.path.join(global_path, \"combined\"), exist_ok=True)\n",
    "out_path = os.path.join(global_path, \"combined\", f\"{patient_id}_combined.json\")\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(combined, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "print(f\"Wrote combined JSON â†’ {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05a40b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_html(data: dict) -> str:\n",
    "    \"\"\"Convert summary JSON (supports multiple schema versions) into styled HTML.\"\"\"\n",
    "\n",
    "    patient_id = _extract_field(\n",
    "        data, [\"patient_ID\", \"patient_id\", (\"assistant_structured_report\", \"patient_ID\")], \"Unknown\"\n",
    "    )\n",
    "    disease = _extract_field(\n",
    "        data, [\"disease_type\", (\"assistant_structured_report\", \"disease_type\")], \"Unknown\"\n",
    "    )\n",
    "    drug = _extract_field(\n",
    "        data,\n",
    "        [\"recomended_drug_name\", \"recommended_drug_name\", (\"assistant_structured_report\", \"recomended_drug_name\")],\n",
    "        \"N/A\",\n",
    "    )\n",
    "    drug_info = _extract_field(\n",
    "        data,\n",
    "        [\"info_on_recommended_drug\", (\"assistant_structured_report\", \"info_on_recommended_drug\"), \"assistant_free_text_report\"],\n",
    "        \"\",\n",
    "    )\n",
    "    decision = _extract_field(\n",
    "        data,\n",
    "        [\"decision_making_process\", (\"assistant_structured_report\", \"decision_making_process\"), \"assistant_free_text_report\"],\n",
    "        \"\",\n",
    "    )\n",
    "\n",
    "    # --- HTML ---\n",
    "    html_parts = [\n",
    "        \"<html><head><meta charset='utf-8'>\",\n",
    "        \"<style>\",\n",
    "        \"body { font-family: 'Segoe UI', Tahoma, sans-serif; margin: 40px; background: #f9f9f9; color: #2c3e50; }\",\n",
    "        \"header { background: #004080; color: white; padding: 20px; border-radius: 8px; }\",\n",
    "        \"header h1 { margin: 0; font-size: 26px; }\",\n",
    "        \"header p { margin: 5px 0; font-size: 16px; }\",\n",
    "        \"section { background: white; padding: 20px; margin-top: 20px; border-radius: 8px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }\",\n",
    "        \"h2 { color: #004080; margin-top: 0; border-bottom: 2px solid #004080; padding-bottom: 6px; }\",\n",
    "        \"h3 { color: #16a085; margin-top: 18px; }\",\n",
    "        \"p, li { font-size: 15px; line-height: 1.6; }\",\n",
    "        \"ul { margin: 0.25rem 0 0.75rem 1.25rem; }\",\n",
    "        \"pre { background: #f4f6f7; padding: 12px; border-left: 4px solid #004080; border-radius: 4px; white-space: pre-wrap; font-size: 14px; }\",\n",
    "        \"</style></head><body>\",\n",
    "\n",
    "        \"<header>\",\n",
    "        \"<h1>Patient Drug Response Prediction Report</h1>\",\n",
    "        f\"<p><strong>Patient ID:</strong> {patient_id}</p>\",\n",
    "        f\"<p><strong>Disease:</strong> {disease}</p>\",\n",
    "        \"</header>\",\n",
    "\n",
    "        \"<section>\",\n",
    "        \"<h2>Recommended Treatment</h2>\",\n",
    "        f\"<p><strong>Drug Regimen:</strong> {drug}</p>\",\n",
    "        \"</section>\",\n",
    "\n",
    "        \"<section>\",\n",
    "        \"<h2>Clinical Considerations</h2>\",\n",
    "        format_clinical_text(drug_info),\n",
    "        \"</section>\",\n",
    "\n",
    "        \"<section>\",\n",
    "        \"<h2>Decision Rationale</h2>\",\n",
    "        format_decision_text(decision),\n",
    "        \"</section>\",\n",
    "\n",
    "        \"</body></html>\",\n",
    "    ]\n",
    "    return \"\\n\".join(html_parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43fd315",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "json_to_html() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     10\u001b[39m     patient_json = _json.loads(patient_json)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Save to file\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mjson_to_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatient_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreports/MM082_report.html\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: json_to_html() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "Outpath = \"/Users/SJp/Documents/project_local/VIB-LLM-SS/ml-summerschool-2025/topic-1_data-integration-and-llms/project/combined/MM082_combined.json\"\n",
    "\n",
    "# Load JSON (fix double-encoded JSON if needed)\n",
    "with open(Outpath, \"r\") as f:\n",
    "    patient_json = json.load(f)\n",
    "if isinstance(patient_json, str):\n",
    "    import json as _json\n",
    "    patient_json = _json.loads(patient_json)\n",
    "\n",
    "\n",
    "# Save to file\n",
    "json_to_html(patient_json, \"reports/MM082_report.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-vib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
